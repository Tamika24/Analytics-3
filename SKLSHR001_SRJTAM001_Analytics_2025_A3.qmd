---
title: "Analytics_assignment3"
format: pdf
editor: visual
---

```{r, warning=FALSE, message=FALSE}
#| include: false
library(cluster)
library(NbClust)
library(fpc)
library(knitr)
library(GGally)
library(parallel)
library(corrplot)
```

# Question 1 : EDA

a\)

```{r, warning=FALSE, message=FALSE}
#| include: false

df <- read.table("STA4026_Assignment_Clustering.txt",
                 header = FALSE)
#dimension of dataset
dim <- dim(df)
kable(dim, col.names = "Dimensions", caption = "The dimensions of the data set")
#data types
kable(str(df), caption= "The structure of the data set")
#missing values
kable(colSums(is.na(df)), caption ="The number of missing values in each column")
#quartiles of dataset
kable(summary(df), caption="A summary of the data statistics")
# IQRs
iqr.v1 <- IQR(df$V1)
iqr.v2 <- IQR(df$V2)
```

In order to gather descriptive statistics for the data, prior checks were conducted: no missing values were found in the dataset and all data entries were of "integer" type. Thus, no data cleaning was necessary. The results from our descriptive analysis is outlined below.

```{r}
#|label: tbl-dscrp
#| echo: false
kable(summary(df), caption="Tabular summary of the descriptive statistics of the data")     
```

The dataset contains 5000 observations of 2 numeric variables, both containing all integer values.

Variable 1 (V1) values ranges from 89606 to 932954. The median is slightly above the mean, which hints at a mild left-skew (a few low values pulling the mean down). Variable 2 (V2) values have a much wider range, from 9597 to 977215. The mean is slightly above the median, suggesting a slight right-skew (a few high values pulling the mean up).

Even though their IQRs are similar, the variables have quite different ranges and means, implying a need for standardization.

However, in both cases, the medians are relatively near their respective means, and are roughly centered between the 1st and 3rd quartiles for each variable. This indicates that the core of the distributions are roughly symmetric.

b\)

Euclidean distance is used as the primary distance metric. This is due to several reasons. Both variables are continuous and numeric. Additionally, despite V2 having a much wider range than V1, the variables are on fairly comparable scales after standardization and will consequently contribute equally to the distance metric. Furthermore, the data is roughly symmetric. This implies balanced variation around the mean, such that the mean is a good central point. Euclidean distance is centered around the mean — so it behaves well under symmetry.

c\)

To explore potential relationships and structure in the data, a pairs plot was constructed using the two standardized variables, V1 and V2.

```{r, warning=FALSE, message=FALSE}
#| echo: false
#| caption: "Pair plot showing univariate and bivariate distributions for V1 and V2"
df_scaled <- scale(df)
ggpairs(as.data.frame(df_scaled))

```

The univariate distribution for V1 is clearly multi-modal, with at least 3 distinct high-density peaks roughly situated near quartile 1, the median and quartile 2, separated by shallow troughs. This suggests that there are three natural subgroups in V1, which can thus form 3 clusters.

In contrast, V2’s univariate density is right‐skewed, with most observations clustered near the center and a long tail stretching out to the right towards the maximum. It is largely bimodal with at least 2 distinct high-density peaks situated roughly around quartile 1 and quartile 3, and separated by a discerningly low trough. Thus, two clusters can be formed in this regard.

The pairwise scatterplot forms a roughly elliptical, symmetric cloud centered at (0,0) with no clear diagonal stretch. As expected the correlation between V1 and V2 is nearly zero, indicating that there is an extremely weak positive linear relationship between the variables- essentially no linear relationship and thus both variables contribute independently to the clustering structure. Because there is visually no distinct groups in this plot, this indicates that the clustering structure is not aligned along a single axis. This means the data doesn’t show a clear vertical or horizontal separation and so might be split diagonally or in more complex patterns. This suggests clustering requires looking at combinations of both variables, not just one.

d\)

```{r, warning=FALSE, message= FALSE}
#| echo: false
dist_matrix <- dist(df_scaled, method="euclidean")
d_vals <- as.vector(dist_matrix)
hist(d_vals, breaks=50, main="Pairwise distances", xlab="Distance")

```

The distance histogram clearly shows a single, broad peak centered around 1–2.2. This suggests the typical distance between any two observations once both variables are on the same scale. It suggests that if you pick two random data points, they’re most likely to be about two standard deviations apart overall. The bars taper off into a long right‐hand tail reaching out to about 5, which indicates a handful of point-pairs that are far apart (either true outliers or members of very distinct subgroups). At the extreme left, the histogram shape indicates that there are a few nearly zero distances corresponding to observations that are virtually identical on V1 and V2, meaning they would always merge immediately in a hierarchical tree or sit in the same k‐means cluster. Altogether, this moderate spread, neither all bunched up or wildly dispersed, suggests clustering algorithms will have enough contrast to pull apart dense regions.

e\)

We will compute the average distance for each observation i to all other points in the distance matrix:

```{r, message=FALSE, warning=FALSE}
#####USING AVG DIST AND 2 STAND DEV TO GET OUTLIERS
avg_d   <- rowMeans(as.matrix(dist_matrix))
mu      <- mean(avg_d)
sigma   <- sd(avg_d)
outliers <- which(avg_d > mu + 2*sigma)       # those beyond 2 SD above the mean
outliers_top10 <- head(order(avg_d, decreasing=TRUE), 10)

#####USING BOXPLOTS HERE, NOT SURE WHICH WAY TO DO IT
# Compute each point’s average distance
d_mat     <- as.matrix(dist(df_scaled))
avg_d     <- rowMeans(d_mat)

# Boxplot the average distances
boxplot(avg_d, main="Avg. Distance to Other Points",
        ylab="Avg Euclidean Distance")

#Identify those beyond the upper whisker (= Q3 + 1.5*IQR)
stats     <- boxplot.stats(avg_d)$stats    # [1]=min [2]=Q1 [3]=med [4]=Q3 [5]=max
upper_cut <- stats[4] + 1.5*(stats[4] - stats[2])
outliers  <- which(avg_d > upper_cut)
length(outliers)  # how many we found

#If more than 10, take the top 10 by avg_d
outliers_top10 <- head(order(avg_d[outliers], decreasing=TRUE), 10)
outliers[outliers_top10]

```

The 10 observations with the largest average distance are our outliers. These points are much further from the bulk of the data. Each of these observations are on average, more than 2 standard deviations farther away from all other observations than a "normal" observation. This could influence the clustering algorithms because the k-means centroids will be pulled towards any of these extreme values and hierarchical merges will bind them at very high heights.

f\)

```{r, message=FALSE, warning=FALSE}
#Compute the correlation matrix on the standardized data
cor_mat <- cor(df_scaled)
kable(cor_mat, caption="The correlation matrix on the standardized data")

#Visualise
corrplot(
  cor_mat,
  method     = "circle",        # or "shade"/"pie" etc for the shapes
  addCoef.col= "black",         # draw the correlation numbers in black
  number.cex = 0.8,             # shrink the text size if needed
  tl.cex     = 1.0,             # text size for variable names
  title      = "Correlation Matrix with Coefficients",
  mar        = c(0,0,1,0)       # make room for the title
)

```

The diagonals are 1 because each variable is perfectly correlated with itself. The off-diagonals represent the association between the 2 variables. As we can see, the correlation between variable 1 and 2 is 0.07. The low correlation suggests that each variable has their own, independent information which influences the data set and suggests no redundancy. These variables won't dominate each other when clustering. Due to the variables being slightly uncorrelated, there would be no need to drop one or decorrelate them as the euclidean distance on the standardized data will treat the variables fairly.

g\)

Yes, we would standardize the data before computing any euclidean distances which is the distance metric we picked in the beginning. We would do this because the raw scales of the variables span hundreds of thousands, so this will drastically influence our clustering algorithms. The euclidean distance is calculated as the square root of the sum of the squares of the differences between corresponding coordinates. Without standardization, whichever variable has the largest absolute spread will dominate and lead to skewed results. By standardizing, each variable has equal contribution. Since both variables showed similar IQRs and no extreme skew in their middle ranges, standardizing simply aligns their scales without distorting the multi modal pattern in V1 or the slight skew in V2, ensuring our clustering sees genuine structure, not scale artifacts.

# Question 2

a\)

```{r, cache=TRUE, message=FALSE, warning=FALSE}
#Compute average silhouette for k-means
sil_km <- sapply(2:20, function(K) {
  km  <- kmeans(df_scaled, centers = K, nstart = 50, iter.max = 100)
  sil <- silhouette(km$cluster, dist_matrix)
  mean(sil[, "sil_width"])
})

#Compute average silhouette for k-medoids (PAM)
sil_pam <- sapply(2:20, function(K) {
  pamc <- pam(dist_matrix, k = K, diss = TRUE)
  mean(pamc$silinfo$widths[, "sil_width"])
})

#Plot them side by side
par(mfrow = c(1,2))
plot(2:20, sil_km, type = "b",
     xlab = "K", ylab = "Avg. silhouette (k-means)",
     main = "k-means silhouette")
plot(2:20, sil_pam, type = "b",
     xlab = "K", ylab = "Avg. silhouette (k-medoids)",
     main = "k-medoids silhouette")
par(mfrow = c(1,1))
```

Both the k-means and k-medoids silhouette curves exhibit two natural “peaks” in the range K=2–20. First, there is a clear local maximum at k=3 (average silhouette ≈ 0.40 for both methods), indicating that three clusters capture the most prominent division in the data, splitting into two clusters loses cohesion, while four or five clusters start to over fragment the main groups. Beyond k=8, the silhouette steadily rises again, peaking around k=15–16 (≈ 0.47 for k-means, ≈ 0.48 for k-medoids), which suggests a much finer partition yields the tightest, most well-separated clusters, but that many clusters may be over‐splitting. Therefore, based on average silhouette alone, two sensible choices emerge for both algorithms: a coarse segmentation at k=3 and a fine-grained segmentation at k=15 (or 16), depending on whether we want to prioritize broader overview or maximum cluster compactness.

b\)

Here, we've chosen to demonstrate the sensitivity of the selected number of clusters with k=15-16 (It's assumed from the question that we should chose one selected number of clusters from the previous question and use that in this question)

```{r, cache=TRUE, warning=FALSE, message=FALSE}
# Your chosen K values and number of replicates
k_vals <- c(15, 16)
runs   <- 100

# Define a function that does 'runs' single-start k-means
# and returns the vector of mean silhouette widths
kmeans_run <- function(dat, k, runs) {
  replicate(runs, {
    #run k-means with a single random start
    km <- kmeans(dat,
                 centers  = k,
                 nstart   = 1,
                 iter.max = 100)
    #compute its silhouette (on the precomputed dist)
    ss <- silhouette(km$cluster, dist_matrix)
    #return the average silhouette width
    mean(ss[, "sil_width"])
  })
}

#Run it for each K
sil_results <- lapply(k_vals, function(k) {
  sils <- kmeans_run(df_scaled, k, runs)
  # Quickly report summary stats
  cat("\n--- K =", k, "---\n")
  print(summary(sils))
  cat("SD =", sd(sils), "\n")
  # Return the raw vector for plotting
  sils
})
names(sil_results) <- paste0("K", k_vals)

#Visualise side by side
par(mfrow = c(1, length(k_vals)))
for(i in seq_along(k_vals)) {
  k <- k_vals[i]
  sils <- sil_results[[i]]
  boxplot(sils,
          main = paste("k-means avg. silhouette\nK =", k),
          ylab = "Avg. silhouette")
}
par(mfrow = c(1,1))
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))
# Density for K = 15
d15 <- density(sil_results[["K15"]])
plot(d15,
     main = "Density of Avg. Silhouette\nk-means (K = 15)",
     xlab = "Avg. silhouette",
     ylab = "Density",
     lwd  = 2)
abline(v = mean(sil_results[["K15"]]), col = "blue", lwd = 1, lty = 2)
abline(v = median(sil_results[["K15"]]), col = "red",  lwd = 1, lty = 2)
legend("topright",
       legend = c("Mean", "Median"),
       col    = c("blue", "red"),
       lwd    = 1,
       lty    = 2,
       cex    = 0.8)

# Density for K = 16
d16 <- density(sil_results[["K16"]])
plot(d16,
     main = "Density of Avg. Silhouette\nk-means (K = 16)",
     xlab = "Avg. silhouette",
     ylab = "Density",
     lwd  = 2)
abline(v = mean(sil_results[["K16"]]), col = "blue", lwd = 1, lty = 2)
abline(v = median(sil_results[["K16"]]), col = "red",  lwd = 1, lty = 2)
legend("topright",
       legend = c("Mean", "Median"),
       col    = c("blue", "red"),
       lwd    = 1,
       lty    = 2,
       cex    = 0.8)

par(mfrow = c(1, 1))
#Pick the two best runs for each K
best_runs <- lapply(sil_results, function(sils) {
  idx <- order(sils, decreasing = TRUE)[1:2]
  data.frame(run = idx, sil = sils[idx])
})
names(best_runs) <- paste0("K", k_vals)

best_runs


```

For k=15 over 100 single-start k-means runs, the average silhouette widths ranged from about 0.405 to 0.479, with a median of 0.468 and a mean of 0.462 (SD ≈ 0.015). For k=16, the range was roughly 0.433–0.476, median 0.473, mean 0.467 (SD ≈ 0.011). In both cases the interquartile ranges (Q3–Q1) are very narrow (\~0.016 for k=15, \~0.011 for k=16), indicating that most random starts yield silhouettes within 1–2% of their respective medians.

Despite the higher complexity of 15 or 16 clusters, k-means remains reasonably stable, the SDs are small relative to the means, and the majority of runs cluster within a tight band around the median silhouette. The slightly larger spread for k=15 (SD 0.015 vs. 0.011) suggests k=16 is marginally more robust to initialization. We would pick the two runs with the highest silhouette widths in each case as our “optimal” initializations, but given the low variability, any run in the top quartile would produce nearly the same clustering quality.

The boxplot for k=15 shows the IQR lies between about 0.46 and 0.48 with no outliers, meaning nearly all starts yield very similar cluster quality. For k=16, the IQR is equally narrow but a few dots around 0.43–0.44 flag rare runs that fell into suboptimal basins. Because both boxes are short relative to the silhouette scale and their medians sit near the top of each IQR, we conclude that k-means at these k values are highly robust to initialization.

```{r, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(123)
k_vals  <- c(15, 16)
n_runs  <- 100
n_samps <- 5   # number of subsamples per CLARA run

#Function to run CLARA with proper arguments and return avg silhouette
clara_run <- function(dat, diss_mat, k, runs, samples = 5) {
  replicate(runs, {
    # 'samples' is the number of subsamples; clara decides sampsize automatically
    clara_res <- clara(dat, k = k, samples = samples, pamLike = TRUE)
    sil       <- silhouette(clara_res$clustering, diss_mat)
    mean(sil[, "sil_width"])
  })
}

#Run sensitivity for each K, summarise and store results
sil_clara_results <- lapply(k_vals, function(k) {
  sils <- clara_run(df_scaled, dist_matrix, k, n_runs, samples = n_samps)
  cat("\n--- K =", k, "---\n")
  print(summary(sils))
  cat("SD =", round(sd(sils), 4), "\n")
  sils
})
names(sil_clara_results) <- paste0("K", k_vals)

#Boxplots side‐by‐side
par(mfrow = c(1, length(k_vals)), mar = c(4, 4, 2, 1))
for(i in seq_along(k_vals)) {
  k    <- k_vals[i]
  sils <- sil_clara_results[[i]]
  boxplot(sils,
          main  = paste("CLARA silhouette\nK =", k),
          ylab  = "Avg. silhouette",
          range = 0)
}
par(mfrow = c(1, 1))

par(mfrow = c(1, length(k_vals)), mar = c(4,4,2,1))

for(i in seq_along(k_vals)) {
  k    <- k_vals[i]
  sils <- sil_clara_results[[i]]
  
  # Compute density
  d <- density(sils)
  
  # Plot density
  plot(d,
       main = paste("CLARA silhouette density\nK =", k),
       xlab = "Avg. silhouette",
       ylab = "Density",
       lwd  = 2)
  
  # Mark mean & median
  abline(v = mean(sils), col = "blue", lty = 2, lwd = 1)
  abline(v = median(sils), col = "red",  lty = 2, lwd = 1)
  
  # Legend
  legend("topright",
         legend = c("Mean", "Median"),
         col    = c("blue", "red"),
         lty    = 2,
         lwd    = 1,
         cex    = 0.8)
}

par(mfrow = c(1,1))
#Identify the two best runs (highest silhouette) for each K
best_clara <- lapply(sil_clara_results, function(sils) {
  idx <- order(sils, decreasing = TRUE)[1:2]
  data.frame(run       = idx,
             silhouette = round(sils[idx], 4))
})
names(best_clara) <- paste0("K", k_vals)

best_clara


```

For both k=15 and k=16, all 100 CLARA runs returned the exact same average silhouette widths—0.4416 for k=15 and 0.4368 for k=16, meaning the minimum, first quartile, median, mean, third quartile and maximum all coincide, and the standard deviation is zero. The resulting boxplots collapse to a single line and the density estimates form a degenerate smooth bump at that one value.

This complete lack of spread tells us that, under CLARA’s subsampling (with 5 subsamples each of size (n/5 ), the medoid‐selection and final clustering are perfectly reproducible: every random subsample leads to the same clustering quality. In practice, that means zero sensitivity to initialization, CLARA is so stable here that there are no “better” or “worse” starts to distinguish.

c\)

These are the summary statistics for a large number of initializations by using parallel execution for each k-means and CLARA:

```{r, cache=TRUE, warning=FALSE, message=FALSE}

set.seed(123)
k_vals <- c(15, 16)
n_runs <- 100

#Worker function for a single k-means silhouette
km_silhouette <- function(i, dat, diss_mat, k) {
  km  <- kmeans(dat, centers = k, nstart = 1, iter.max = 100)
  ss  <- silhouette(km$cluster, diss_mat)
  mean(ss[, "sil_width"])
}

#Worker function for a single CLARA silhouette
clara_silhouette <- function(i, dat, diss_mat, k, samples = 5) {
  cr <- clara(dat, k = k, samples = samples, pamLike = TRUE)
  ss <- silhouette(cr$clustering, diss_mat)
  mean(ss[, "sil_width"])
}

#Launch cluster
ncores <- detectCores() - 1
cl     <- makeCluster(ncores)
clusterExport(cl, c("df_scaled", "dist_matrix", "km_silhouette",
                    "clara_silhouette", "n_runs"))
clusterEvalQ(cl, library(cluster))

#Parallel runs & summaries
results <- lapply(k_vals, function(k) {
  #k-means
  km_sils <- parSapply(cl, 1:n_runs,
                       km_silhouette,
                       dat     = df_scaled,
                       diss_mat= dist_matrix,
                       k       = k)
  #CLARA
  clara_sils <- parSapply(cl, 1:n_runs,
                          clara_silhouette,
                          dat     = df_scaled,
                          diss_mat= dist_matrix,
                          k       = k,
                          samples = 5)
  
  #Summarize
  cat("\n===== K =", k, "===== \n")
  cat("k-means avg. silhouette:\n")
  print(summary(km_sils))
  cat("SD =", round(sd(km_sils), 4), "\n\n")
  
  cat("CLARA avg. silhouette:\n")
  print(summary(clara_sils))
  cat("SD =", round(sd(clara_sils), 4), "\n")
  
  list(kmeans = km_sils, clara = clara_sils)
})
names(results) <- paste0("K", k_vals)

stopCluster(cl)
```

d\)

```{r, warning=FALSE, message=FALSE, cache=TRUE}
set.seed(123)
gap_km_fast <- clusGap(
  df_scaled,
  FUN      = kmeans,
  K.max    = 20,
  B        = 10,
  verbose  = FALSE,
  detectCores()-1
)
plot(gap_km_fast,main = "Gap Statistic vs. Number of Clusters", xlab = "K", ylab = "Gap statistic")
print(gap_km_fast, method="firstSEmax")

```

The gap statistic peaks at k=3 (gap ≈ 2.88) and then steadily falls, even where the silhouette scores were highest at k=15 (gap ≈ 2.14) and K=16K=16K=16 (gap ≈ 1.97). Silhouette, in contrast, keeps rising to favor many small, tight clusters, peaking at 15–16 (≈ 0.47). You can choose 3 for a broad, noise‐robust solution or 15–16 for a fine‐grained, highly cohesive one.
